{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "import operator\n",
    "\n",
    "KEEP_WORDS = set([\n",
    "  'king', 'man', 'queen', 'woman',\n",
    "  'italy', 'rome', 'france', 'paris',\n",
    "  'london', 'britain', 'england',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences():\n",
    "  # returns 57340 of the Brown corpus\n",
    "  # each sentence is represented as a list of individual string tokens\n",
    "    return brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_with_word2idx():\n",
    "  sentences = get_sentences()\n",
    "  indexed_sentences = []\n",
    "\n",
    "  i = 2\n",
    "  word2idx = {'START': 0, 'END': 1}\n",
    "  for sentence in sentences:\n",
    "    indexed_sentence = []\n",
    "    for token in sentence:\n",
    "      token = token.lower()\n",
    "      if token not in word2idx:\n",
    "        word2idx[token] = i\n",
    "        i += 1\n",
    "\n",
    "      indexed_sentence.append(word2idx[token])\n",
    "    indexed_sentences.append(indexed_sentence)\n",
    "\n",
    "  print (\"Vocab size:\", i)\n",
    "  return indexed_sentences, word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences_with_word2idx_limit_vocab(n_vocab=2000, keep_words=KEEP_WORDS):\n",
    "  sentences = get_sentences()\n",
    "  indexed_sentences = []\n",
    "\n",
    "  i = 2\n",
    "  word2idx = {'START': 0, 'END': 1}\n",
    "  idx2word = ['START', 'END']\n",
    "\n",
    "  word_idx_count = {\n",
    "    0: float('inf'),\n",
    "    1: float('inf'),\n",
    "  }\n",
    "\n",
    "  for sentence in sentences:\n",
    "    indexed_sentence = []\n",
    "    for token in sentence:\n",
    "      token = token.lower()\n",
    "      if token not in word2idx:\n",
    "        idx2word.append(token)\n",
    "        word2idx[token] = i\n",
    "        i += 1\n",
    "\n",
    "      # keep track of counts for later sorting\n",
    "      idx = word2idx[token]\n",
    "      word_idx_count[idx] = word_idx_count.get(idx, 0) + 1\n",
    "\n",
    "      indexed_sentence.append(idx)\n",
    "    indexed_sentences.append(indexed_sentence)\n",
    "\n",
    "\n",
    "\n",
    "  # restrict vocab size\n",
    "\n",
    "  # set all the words I want to keep to infinity\n",
    "  # so that they are included when I pick the most\n",
    "  # common words\n",
    "  for word in keep_words:\n",
    "    word_idx_count[word2idx[word]] = float('inf')\n",
    "\n",
    "  sorted_word_idx_count = sorted(word_idx_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "  word2idx_small = {}\n",
    "  new_idx = 0\n",
    "  idx_new_idx_map = {}\n",
    "  for idx, count in sorted_word_idx_count[:n_vocab]:\n",
    "    word = idx2word[idx]\n",
    "    print(word, count)\n",
    "    word2idx_small[word] = new_idx\n",
    "    idx_new_idx_map[idx] = new_idx\n",
    "    new_idx += 1\n",
    "  # let 'unknown' be the last token\n",
    "  word2idx_small['UNKNOWN'] = new_idx \n",
    "  unknown = new_idx\n",
    "\n",
    "  assert('START' in word2idx_small)\n",
    "  assert('END' in word2idx_small)\n",
    "  for word in keep_words:\n",
    "    assert(word in word2idx_small)\n",
    "\n",
    "  # map old idx to new idx\n",
    "  sentences_small = []\n",
    "  for sentence in indexed_sentences:\n",
    "    if len(sentence) > 1:\n",
    "      new_sentence = [idx_new_idx_map[idx] if idx in idx_new_idx_map else unknown for idx in sentence]\n",
    "      sentences_small.append(new_sentence)\n",
    "\n",
    "  return sentences_small, word2idx_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

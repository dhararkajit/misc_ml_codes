{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Open Machine Learning Course\n",
    "<center>Author: [Yury Kashnitsky](https://www.linkedin.com/in/festline). Translated and edited by [Christina Butsko](https://www.linkedin.com/in/christinabutsko/), [Nerses Bagiyan](https://www.linkedin.com/in/nersesbagiyan/), [Yulia Klimushina](https://www.linkedin.com/in/yuliya-klimushina-7168a9139), and [Yuanyuan Pao](https://www.linkedin.com/in/yuanyuanpao/).\n",
    "\n",
    "This material is subject to the terms and conditions of the license [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Free use is permitted for any non-comercial purpose with an obligatory indication of the names of the authors and of the source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Topic 4. Linear Classification and Regression\n",
    "## <center> Part 4. Where Logistic Regression Is Good and Where It's Not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of IMDB movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for a little practice! We want to solve the problem of binary classification of IMDB movie reviews. We have a training set with marked reviews, 12500 reviews marked as good, another 12500 bad. Here, it's not easy to get started with machine learning right away because we don't have the matrix $X$; we need to prepare it. We will use a simple approach: bag of words model. Features of the review will be represented by indicators of the presence of each word from the whole corpus in this review. The corpus is the set of all user reviews. The idea is illustrated by a picture\n",
    "\n",
    "<img src=\"../../img/bag_of_words.svg\" width=80%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To get started, download the dataset [here](http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz) (direct download link). The dataset is briefly described [here](http://ai.stanford.edu/~amaas/data/sentiment/). There are 12.5k of good and bad reviews in the test and training sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the path to the file\n",
    "reviews_train = load_files(\"C:/Users/Arkajit/Anaconda2/envs/TFENV2/data/aclImdb/train\")\n",
    "text_train, y_train = reviews_train.data, reviews_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in training data: 75000\n",
      "[12500 12500 50000]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents in training data: %d\" % len(text_train))\n",
    "print(np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in test data: 25000\n",
      "[12500 12500]\n"
     ]
    }
   ],
   "source": [
    "# change the path to the file\n",
    "reviews_test = load_files(\"C:/Users/Arkajit/Anaconda2/envs/TFENV2/data/aclImdb/test\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "print(\"Number of documents in test data: %d\" % len(text_test))\n",
    "print(np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here are a few examples of the reviews.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Amount of disappointment I am getting these days seeing movies like Partner, Jhoom Barabar and now, Heyy Babyy is gonna end my habit of seeing first day shows.<br /><br />The movie is an utter disappointment because it had the potential to become a laugh riot only if the d\\xc3\\xa9butant director, Sajid Khan hadn't tried too many things. Only saving grace in the movie were the last thirty minutes, which were seriously funny elsewhere the movie fails miserably. First half was desperately been tried to look funny but wasn't. Next 45 minutes were emotional and looked totally artificial and illogical.<br /><br />OK, when you are out for a movie like this you don't expect much logic but all the flaws tend to appear when you don't enjoy the movie and thats the case with Heyy Babyy. Acting is good but thats not enough to keep one interested.<br /><br />For the positives, you can take hot actresses, last 30 minutes, some comic scenes, good acting by the lead cast and the baby. Only problem is that these things do not come together properly to make a good movie.<br /><br />Anyways, I read somewhere that It isn't a copy of Three men and a baby but I think it would have been better if it was.\"\n"
     ]
    }
   ],
   "source": [
    "print(text_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1] # bad review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'The future, we are told, are what we make of it. Philip K. Dick did not want to take that chance, so he wrote many many many short stories about the future of man and where we, as a society, were headed. Blade Runner, Total Recall, Paycheck, Screamers, and Minority Report are all short stories written by Dick about the future that have been turned into a movie, and most have a less than enthusiastic view of where we are headed. In Minority Report, we see the effects of predicting the future to the point of crimes are prevented by arresting murderers before they kill. If that does not appear logical, there is a quick little scene early in the movie that addresses those concerns, and on the surface makes sense. Tom Cruise plays the Washington, DC pre-crime chief, John Anderton, who runs the investigators who rely on 3 scientifically engineered beings who can see murders before they happen. The system, of course, raises civil liberty issues, but seems to work perfectly, that is until Anderton is fingered for a murder. The rest of the movie, Anderton tries to not only prove that he is innocent, but also that he was set up, possibly by an oily Department of Justice figure who is investigating Precrime before it goes national after an election, played by Colin Farrell. Directed by Steven Spielberg, Minority Report plays as both a \"Whodunnit?\" and a futuristic exercise of science fiction. Much time was spent on designing the Washington, DC of the 2050s, including cars that run on magnets, virtual reality stations, and much more throughout the film. The most interesting design is of the \"sick sticks\" used by cops to bring down criminals. The blueish tint given to the film also gives us a cold feeling, a future that is not as loving or as hospitable as the time we live in, another trait of a Dick story. A wonderful movie the works for both the crime buff and the science fiction fan.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[2] # good review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Count of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, we will create a dictionary of all the words using CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124255"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "cv.fit(text_train)\n",
    "\n",
    "len(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you look at the examples of \"words\" (let's call them tokens), you can see that we have omitted many of the important steps in text processing (automatic text processing can itself be a completely separate series of articles).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000', '0000000000000000000000000000000001', '0000000000001', '000000001', '000000003', '00000001', '000001745', '00001', '0001', '00015', '0002', '0007', '00083', '000ft', '000s', '000th', '001', '002', '003', '003830', '004', '005', '006', '007', '0079', '008', '0080', '0083', '009', '0091042', '0093638', '00am', '00o', '00pm', '00s', '01', '015', '019', '01p', '01pm', '02', '020410', '022', '0267', '029', '03', '0312', '039']\n",
      "['heathen', 'heathens', 'heather', 'heatherbennett', 'heatherly', 'heathers', 'heatherton', 'heathkit', 'heathrow', 'heating', 'heatman', 'heatmiser', 'heaton', 'heats', 'heatseeker', 'heatwave', 'heave', 'heaved', 'heaven', 'heavenlier', 'heavenliness', 'heavenly', 'heavens', 'heavenward', 'heaves', 'heavier', 'heavies', 'heaviest', 'heavily', 'heaviness', 'heaving', 'heavy', 'heavyarms', 'heavyhanded', 'heavys', 'heavyset', 'heavyweight', 'heavyweights', 'heber', 'hebert', 'hebetude', 'hebner', 'hebraic', 'hebrew', 'hebrews', 'hebrides', 'hebron', 'heche', 'hecht', 'hechtdom']\n"
     ]
    }
   ],
   "source": [
    "print(cv.get_feature_names()[:50])\n",
    "print(cv.get_feature_names()[50000:50050])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Secondly, we are encoding the sentences from the training set texts with the indices of incoming words. We'll use the sparse format.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<75000x124255 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10359806 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = cv.transform(text_train)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's see how our transformation worked**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'When celebrating the cycle-of-life \"Cabaret\" sits on the spice shelf.<br /><br />A town which is occupied by a sausage-factory which comes to life when the meat-plant workers come in for rest and relaxation and perform the lessons and morality-tales which are learned from safe and unsafe handling of food and objects around different life-forms.<br /><br />That sex, love, and family must sometimes be sacrificed to perform and follow the foot-lights of the stage.<br /><br />War is sometimes a bi-product of military-personal racing to their favourite performance venue(s) where \"other\" children and wives lived and worked in Europe and especially in Germany.<br /><br />The cycle-of-life is the call for many to chase the memory and contact with those people and places which made them excel at something, namely war.<br /><br />That in the end the only way to keep some of the universe\\'s life-forms from racing to a celebration of war as depicted in other films such as \"Wood Stock\" and \"Schindler\\'s List\" was to electronically engage and control the minds and movements of those who chase a special moment based on the destruction of others.<br /><br />The difference in the end was the sausage and the workers.<br /><br />Which are you?'\n"
     ]
    }
   ],
   "source": [
    "print(text_train[19726])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5618,   7054,   7369,   7611,   8095,  10438,  10846,  12304,\n",
       "        14846,  16930,  17043,  17335,  19129,  19130,  19874,  20445,\n",
       "        22862,  22895,  24215,  24427,  26983,  29377,  29850,  30506,\n",
       "        30509,  35168,  35960,  36083,  37034,  37373,  37740,  38685,\n",
       "        38949,  39465,  40411,  41637,  41697,  41728,  41777,  42019,\n",
       "        42941,  44847,  48974,  54396,  56764,  59788,  63251,  63841,\n",
       "        64209,  64299,  64681,  64804,  65659,  66706,  67864,  69694,\n",
       "        70191,  71254,  71442,  72560,  73037,  73563,  74308,  74845,\n",
       "        77646,  77840,  78010,  78458,  78529,  79267,  79272,  82072,\n",
       "        82219,  82223,  82444,  83765,  83852,  86367,  88401,  90774,\n",
       "        91678,  94532,  94621,  95632,  96154,  98076,  98732, 100389,\n",
       "       102253, 102284, 102290, 102977, 103177, 103986, 104950, 106183,\n",
       "       108285, 109890, 109916, 109969, 109985, 110367, 111180, 111939,\n",
       "       115549, 116025, 117589, 119400, 119561, 119773, 120484, 120491,\n",
       "       120516, 120719, 121444, 121518, 121743, 121919, 121921, 123190])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[19726].nonzero()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0]),\n",
       " array([  5618,   7054,   7369,   7611,   8095,  10438,  10846,  12304,\n",
       "         14846,  16930,  17043,  17335,  19129,  19130,  19874,  20445,\n",
       "         22862,  22895,  24215,  24427,  26983,  29377,  29850,  30506,\n",
       "         30509,  35168,  35960,  36083,  37034,  37373,  37740,  38685,\n",
       "         38949,  39465,  40411,  41637,  41697,  41728,  41777,  42019,\n",
       "         42941,  44847,  48974,  54396,  56764,  59788,  63251,  63841,\n",
       "         64209,  64299,  64681,  64804,  65659,  66706,  67864,  69694,\n",
       "         70191,  71254,  71442,  72560,  73037,  73563,  74308,  74845,\n",
       "         77646,  77840,  78010,  78458,  78529,  79267,  79272,  82072,\n",
       "         82219,  82223,  82444,  83765,  83852,  86367,  88401,  90774,\n",
       "         91678,  94532,  94621,  95632,  96154,  98076,  98732, 100389,\n",
       "        102253, 102284, 102290, 102977, 103177, 103986, 104950, 106183,\n",
       "        108285, 109890, 109916, 109969, 109985, 110367, 111180, 111939,\n",
       "        115549, 116025, 117589, 119400, 119561, 119773, 120484, 120491,\n",
       "        120516, 120719, 121444, 121518, 121743, 121919, 121921, 123190]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[19726].nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Third, we will apply the same operations to the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = cv.transform(text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The next step is to train Logistic Regression.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arkajit\\Anaconda2\\envs\\TFENV2\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31min 43s\n",
      "Parser   : 226 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logit = LogisticRegression(n_jobs=-1, random_state=7)\n",
    "logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's look at accuracy on the both the training and the test sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.94999999999999996, 0.20000000000000001)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(logit.score(X_train, y_train), 3), round(logit.score(X_test, y_test), 3),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The coefficients of the model can be beautifully displayed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_coefficients(classifier, feature_names, n_top_features=25):\n",
    "    # get coefficients with large absolute values \n",
    "    coef = classifier.coef_.ravel()\n",
    "    positive_coefficients = np.argsort(coef)[-n_top_features:]\n",
    "    negative_coefficients = np.argsort(coef)[:n_top_features]\n",
    "    interesting_coefficients = np.hstack([negative_coefficients, positive_coefficients])\n",
    "    # plot them\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    colors = [\"red\" if c < 0 else \"blue\" for c in coef[interesting_coefficients]]\n",
    "    plt.bar(np.arange(2 * n_top_features), coef[interesting_coefficients], color=colors)\n",
    "    feature_names = np.array(feature_names)\n",
    "    plt.xticks(np.arange(1, 1 + 2 * n_top_features), feature_names[interesting_coefficients], rotation=60, ha=\"right\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid_scores(grid, param_name):\n",
    "    plt.plot(grid.param_grid[param_name], grid.cv_results_['mean_train_score'],\n",
    "        color='green', label='train')\n",
    "    plt.plot(grid.param_grid[param_name], grid.cv_results_['mean_test_score'],\n",
    "        color='red', label='test')\n",
    "    plt.legend();\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 263318 is out of bounds for axis 1 with size 124255",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-303c0c8a5d68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvisualize_coefficients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-6bd0b9739b3d>\u001b[0m in \u001b[0;36mvisualize_coefficients\u001b[1;34m(classifier, feature_names, n_top_features)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_top_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minteresting_coefficients\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mfeature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_top_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minteresting_coefficients\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrotation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"right\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 263318 is out of bounds for axis 1 with size 124255"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2cf3b3fa048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_coefficients(logit, cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To make our model better, we can optimize the regularization coefficient for the `Logistic Regression`. We'll use `sklearn.pipeline` because `CountVectorizer` should only be applied to the training data (so as to not \"peek\" into the test set and not count word frequencies there). In this case, `pipeline` determines the correct sequence of actions: apply `CountVectorizer`, then train `Logistic Regression`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arkajit\\Anaconda2\\envs\\TFENV2\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20036\n",
      "Wall time: 19min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "text_pipe_logit = make_pipeline(CountVectorizer(), \n",
    "                                LogisticRegression(n_jobs=-1, random_state=7))\n",
    "\n",
    "text_pipe_logit.fit(text_train, y_train)\n",
    "print(text_pipe_logit.score(text_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid_logit = {'logisticregression__C': np.logspace(-5, 0, 6)}\n",
    "grid_logit = GridSearchCV(text_pipe_logit, param_grid_logit, cv=3, n_jobs=-1)\n",
    "\n",
    "grid_logit.fit(text_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's print best $C$ and cv-score using this hyperparameter:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_logit.best_params_, grid_logit.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid_scores(grid_logit, 'logisticregression__C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_logit.score(text_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's do the same with random forest. We see that, with logistic regression, we achieve better accuracy with less effort.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(forest.score(X_test, y_test), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR-Problem\n",
    "Let's now consider an example where linear models are worse.\n",
    "\n",
    "Linear classification methods still define a very simple separating surface - a hyperplane. The most famous toy example of where classes cannot be divided by a hyperplane (or line) with no errors is \"the XOR problem\".\n",
    "\n",
    "XOR is the \"exclusive OR\", a Boolean function with the following truth table:\n",
    "\n",
    "\n",
    "\n",
    "<img src='../../img/XOR_table.gif'>\n",
    "\n",
    "XOR is the name given to a simple binary classification problem in which the classes are presented as diagonally extended intersecting point clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataset\n",
    "rng = np.random.RandomState(0)\n",
    "X = rng.randn(200, 2)\n",
    "y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], s=30, c=y, cmap=plt.cm.Paired);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, one cannot draw a single straight line to separate one class from another without errors. Therefore, logistic regression performs poorly with this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundary(clf, X, y, plot_title):\n",
    "    xx, yy = np.meshgrid(np.linspace(-3, 3, 50),\n",
    "                     np.linspace(-3, 3, 50))\n",
    "    clf.fit(X, y)\n",
    "    # plot the decision function for each datapoint on the grid\n",
    "    Z = clf.predict_proba(np.vstack((xx.ravel(), yy.ravel())).T)[:, 1]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    image = plt.imshow(Z, interpolation='nearest',\n",
    "                           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "                           aspect='auto', origin='lower', cmap='seismic')\n",
    "    contours = plt.contour(xx, yy, Z, levels=[0], linewidths=2,\n",
    "                               linetypes='--')\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=30, c=y, cmap=plt.cm.Paired)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.xlabel(r'$x_1$')\n",
    "    plt.ylabel(r'$x_2$')\n",
    "    plt.axis([-3, 3, -3, 3])\n",
    "    plt.colorbar(image)\n",
    "    plt.title(plot_title, fontsize=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundary(LogisticRegression(), X, y,\n",
    "              \"Logistic Regression, XOR problem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if one were to give polynomial features as an input (here, up to 2 degrees), then the problem is solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_pipe = Pipeline([('poly', PolynomialFeatures(degree=2)), \n",
    "                       ('logit', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundary(logit_pipe, X, y,\n",
    "              \"Logistic Regression + quadratic features. XOR problem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, logistic regression has still produced a hyperplane but in a 6-dimensional feature space $1, x_1, x_2, x_1^2, x_1x_2$ and $x_2^2$. When we project to the original feature space, $x_1, x_2$, the boundary is nonlinear.\n",
    "\n",
    "In practice, polynomial features do help, but it is computationally inefficient to build them explicitly. SVM with the kernel trick works much faster. In this approach, only the distance between the objects (defined by the kernel function) in a high dimensional space is computed, and there is no need to produce a combinatorially large number of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful resources\n",
    "- Medium [\"story\"](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-4-linear-classification-and-regression-44a41b9b5220) based on this notebook\n",
    "- If you read Russian: an [article](https://habrahabr.ru/company/ods/blog/323890/) on Habrahabr with ~ the same material. And a [lecture](https://youtu.be/oTXGQ-_oqvI) on YouTube\n",
    "- A nice and concise overview of linear models is given in the book [“Deep Learning”](http://www.deeplearningbook.org) (I. Goodfellow, Y. Bengio, and A. Courville).\n",
    "- Linear models are covered practically in every ML book. We recommend “Pattern Recognition and Machine Learning” (C. Bishop) and “Machine Learning: A Probabilistic Perspective” (K. Murphy).\n",
    "- If you prefer a thorough overview of linear model from a statistician’s viewpoint, then look at “The elements of statistical learning” (T. Hastie, R. Tibshirani, and J. Friedman).\n",
    "- The book “Machine Learning in Action” (P. Harrington) will walk you through implementations of classic ML algorithms in pure Python.\n",
    "- [Scikit-learn](http://scikit-learn.org/stable/documentation.html) library. These guys work hard on writing really clear documentation.\n",
    "- Scipy 2017 [scikit-learn tutorial](https://github.com/amueller/scipy-2017-sklearn) by Alex Gramfort and Andreas Mueller.\n",
    "- One more [ML course](https://github.com/diefimov/MTH594_MachineLearning) with very good materials.\n",
    "- [Implementations](https://github.com/rushter/MLAlgorithms) of many ML algorithms. Search for linear regression and logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

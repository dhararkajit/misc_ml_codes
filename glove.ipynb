{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.utils import shuffle\n",
    "from word2vec import get_wikipedia_data,  get_sentences_with_word2idx_limit_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _find_analogies(w1, w2, w3, We, word2idx):\n",
    "    king = We[word2idx[w1]]\n",
    "    man = We[word2idx[w2]]\n",
    "    woman = We[word2idx[w3]]\n",
    "    v0 = king - man + woman\n",
    "\n",
    "    def dist1(a, b):\n",
    "        return np.linalg.norm(a - b)\n",
    "    def dist2(a, b):\n",
    "        return 1 - a.dot(b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "    for dist, name in [(dist1, 'Euclidean'), (dist2, 'cosine')]:\n",
    "        min_dist = float('inf')\n",
    "        best_word = ''\n",
    "        for word, idx in word2idx.items():\n",
    "            if word not in (w1, w2, w3):\n",
    "                v1 = We[idx]\n",
    "                d = dist(v0, v1)\n",
    "                if d < min_dist:\n",
    "                    min_dist = d\n",
    "                    best_word = word\n",
    "        print (\"closest match by\", name, \"distance:\", best_word)\n",
    "        print (w1, \"-\", w2, \"=\", best_word, \"-\", w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (<ipython-input-29-64a44d60b51a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-29-64a44d60b51a>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    def find_analogies(w1, w2, w3, concat=True, we_file, w2i_file):\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "def find_analogies(w1, w2, w3, concat=True, we_file, w2i_file):\n",
    "    npz = np.load(we_file)\n",
    "    W1 = npz['arr_0']\n",
    "    W2 = npz['arr_1']\n",
    "\n",
    "    with open(w2i_file) as f:\n",
    "        word2idx = json.load(f)\n",
    "\n",
    "    V = len(word2idx)\n",
    "\n",
    "    if concat:\n",
    "        We = np.hstack([W1, W2.T])\n",
    "        print (\"We.shape:\", We.shape)\n",
    "        assert(V == We.shape[0])\n",
    "    else:\n",
    "        We = (W1 + W2.T) / 2\n",
    "\n",
    "    _find_analogies(w1, w2, w3, We, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glove:\n",
    "    def __init__(self, D, V, context_sz):\n",
    "        self.D = D\n",
    "        self.V = V\n",
    "        self.context_sz = context_sz\n",
    "\n",
    "    def fit(self, sentences, cc_matrix=None, learning_rate=1e-4, reg=0.1, xmax=100, alpha=0.75, epochs=10, gd=False, use_theano=False, use_tensorflow=False):\n",
    "        # build co-occurrence matrix\n",
    "        # paper calls it X, so we will call it X, instead of calling\n",
    "        # the training data X\n",
    "        # TODO: would it be better to use a sparse matrix?\n",
    "        t0 = datetime.now()\n",
    "        V = self.V\n",
    "        D = self.D\n",
    "\n",
    "        if not os.path.exists(cc_matrix):\n",
    "            X = np.zeros((V, V))\n",
    "            N = len(sentences)\n",
    "            print (\"number of sentences to process:\", N)\n",
    "            it = 0\n",
    "            for sentence in sentences:\n",
    "                it += 1\n",
    "                if it % 10000 == 0:\n",
    "                    print (\"processed\", it, \"/\", N)\n",
    "                n = len(sentence)\n",
    "                for i in range(n):\n",
    "                    # i is not the word index!!!\n",
    "                    # j is not the word index!!!\n",
    "                    # i just points to which element of the sequence (sentence) we're looking at\n",
    "                    wi = sentence[i]\n",
    "\n",
    "                    start = max(0, i - self.context_sz)\n",
    "                    end = min(n, i + self.context_sz)\n",
    "\n",
    "                    # we can either choose only one side as context, or both\n",
    "                    # here we are doing both\n",
    "\n",
    "                    # make sure \"start\" and \"end\" tokens are part of some context\n",
    "                    # otherwise their f(X) will be 0 (denominator in bias update)\n",
    "                    if i - self.context_sz < 0:\n",
    "                        points = 1.0 / (i + 1)\n",
    "                        X[wi,0] += points\n",
    "                        X[0,wi] += points\n",
    "                    if i + self.context_sz > n:\n",
    "                        points = 1.0 / (n - i)\n",
    "                        X[wi,1] += points\n",
    "                        X[1,wi] += points\n",
    "\n",
    "                    # left side\n",
    "                    for j in range(start, i):\n",
    "                        wj = sentence[j]\n",
    "                        points = 1.0 / (i - j) # this is +ve\n",
    "                        X[wi,wj] += points\n",
    "                        X[wj,wi] += points\n",
    "\n",
    "                    # right side\n",
    "                    for j in range(i + 1, end):\n",
    "                        wj = sentence[j]\n",
    "                        points = 1.0 / (j - i) # this is +ve\n",
    "                        X[wi,wj] += points\n",
    "                        X[wj,wi] += points\n",
    "\n",
    "            # save the cc matrix because it takes forever to create\n",
    "            np.save(cc_matrix, X)\n",
    "        else:\n",
    "            X = np.load(cc_matrix)\n",
    "\n",
    "        print (\"max in X:\", X.max())\n",
    "\n",
    "        # weighting\n",
    "        fX = np.zeros((V, V))\n",
    "        fX[X < xmax] = (X[X < xmax] / float(xmax)) ** alpha\n",
    "        fX[X >= xmax] = 1\n",
    "\n",
    "        print (\"max in f(X):\", fX.max())\n",
    "\n",
    "        # target\n",
    "        logX = np.log(X + 1)\n",
    "\n",
    "        print (\"max in log(X):\", logX.max())\n",
    "\n",
    "        print (\"time to build co-occurrence matrix:\", (datetime.now() - t0))\n",
    "\n",
    "        # initialize weights\n",
    "        W = np.random.randn(V, D) / np.sqrt(V + D)\n",
    "        b = np.zeros(V)\n",
    "        U = np.random.randn(V, D) / np.sqrt(V + D)\n",
    "        c = np.zeros(V)\n",
    "        mu = logX.mean()\n",
    "\n",
    "        if use_theano:\n",
    "            # initialize weights, inputs, targets placeholders\n",
    "            thW = theano.shared(W)\n",
    "            thb = theano.shared(b)\n",
    "            thU = theano.shared(U)\n",
    "            thc = theano.shared(c)\n",
    "            thLogX = T.matrix('logX')\n",
    "            thfX = T.matrix('fX')\n",
    "\n",
    "            params = [thW, thb, thU, thc]\n",
    "\n",
    "            thDelta = thW.dot(thU.T) + T.reshape(thb, (V, 1)) + T.reshape(thc, (1, V)) + mu - thLogX\n",
    "            thCost = ( thfX * thDelta * thDelta ).sum()\n",
    "\n",
    "            # regularization\n",
    "            thCost += reg*( (thW * thW).sum() + (thU * thU).sum() + (thb * thb).sum() + (thc * thc).sum())\n",
    "\n",
    "            grads = T.grad(thCost, params)\n",
    "\n",
    "            updates = [(p, p - learning_rate*g) for p, g in zip(params, grads)]\n",
    "\n",
    "            train_op = theano.function(\n",
    "                inputs=[thfX, thLogX],\n",
    "                updates=updates,\n",
    "            )\n",
    "\n",
    "        elif use_tensorflow:\n",
    "            # initialize weights, inputs, targets placeholders\n",
    "            tfW = tf.Variable(W.astype(np.float32))\n",
    "            tfb = tf.Variable(b.reshape(V, 1).astype(np.float32))\n",
    "            tfU = tf.Variable(U.astype(np.float32))\n",
    "            tfc = tf.Variable(c.reshape(1, V).astype(np.float32))\n",
    "            tfLogX = tf.placeholder(tf.float32, shape=(V, V))\n",
    "            tffX = tf.placeholder(tf.float32, shape=(V, V))\n",
    "\n",
    "            delta = tf.matmul(tfW, tf.transpose(tfU)) + tfb + tfc + mu - tfLogX\n",
    "            cost = tf.reduce_sum(tffX * delta * delta)\n",
    "            for param in (tfW, tfb, tfU, tfc):\n",
    "                cost += reg*tf.reduce_sum(param * param)\n",
    "\n",
    "            train_op = tf.train.MomentumOptimizer(learning_rate, momentum=0.9).minimize(cost)\n",
    "            init = tf.global_variables_initializer()\n",
    "            session = tf.InteractiveSession()\n",
    "            session.run(init)\n",
    "\n",
    "        costs = []\n",
    "        sentence_indexes = range(len(sentences))\n",
    "        for epoch in range(epochs):\n",
    "            delta = W.dot(U.T) + b.reshape(V, 1) + c.reshape(1, V) + mu - logX\n",
    "            cost = ( fX * delta * delta ).sum()\n",
    "            costs.append(cost)\n",
    "            print (\"epoch:\", epoch, \"cost:\", cost)\n",
    "\n",
    "            if gd:\n",
    "                # gradient descent method\n",
    "\n",
    "                if use_theano:\n",
    "                    train_op(fX, logX)\n",
    "                    W = thW.get_value()\n",
    "                    b = thb.get_value()\n",
    "                    U = thU.get_value()\n",
    "                    c = thc.get_value()\n",
    "\n",
    "                elif use_tensorflow:\n",
    "                    session.run(train_op, feed_dict={tfLogX: logX, tffX: fX})\n",
    "                    W, b, U, c = session.run([tfW, tfb, tfU, tfc])\n",
    "\n",
    "                else:\n",
    "                    # update W\n",
    "                    oldW = W.copy()\n",
    "                    for i in range(V):\n",
    "                        # for j in xrange(V):\n",
    "                        #     W[i] -= learning_rate*fX[i,j]*(W[i].dot(U[j]) + b[i] + c[j] + mu - logX[i,j])*U[j]\n",
    "                        W[i] -= learning_rate*(fX[i,:]*delta[i,:]).dot(U)\n",
    "                    W -= learning_rate*reg*W\n",
    "                    # print \"updated W\"\n",
    "\n",
    "                    # update b\n",
    "                    for i in range(V):\n",
    "                        # for j in xrange(V):\n",
    "                        #     b[i] -= learning_rate*fX[i,j]*(W[i].dot(U[j]) + b[i] + c[j] + mu - logX[i,j])\n",
    "                        b[i] -= learning_rate*fX[i,:].dot(delta[i,:])\n",
    "                    b -= learning_rate*reg*b\n",
    "                    # print \"updated b\"\n",
    "\n",
    "                    # update U\n",
    "                    for j in range(V):\n",
    "                        # for i in xrange(V):\n",
    "                        #     U[j] -= learning_rate*fX[i,j]*(W[i].dot(U[j]) + b[i] + c[j] + mu - logX[i,j])*W[i]\n",
    "                        U[j] -= learning_rate*(fX[:,j]*delta[:,j]).dot(oldW)\n",
    "                    U -= learning_rate*reg*U\n",
    "                    # print \"updated U\"\n",
    "\n",
    "                    # update c\n",
    "                    for j in range(V):\n",
    "                        # for i in xrange(V):\n",
    "                        #     c[j] -= learning_rate*fX[i,j]*(W[i].dot(U[j]) + b[i] + c[j] + mu - logX[i,j])\n",
    "                        c[j] -= learning_rate*fX[:,j].dot(delta[:,j])\n",
    "                    c -= learning_rate*reg*c\n",
    "                    # print \"updated c\"\n",
    "\n",
    "            else:\n",
    "                # ALS method\n",
    "\n",
    "                # update W\n",
    "                # fast way\n",
    "                # t0 = datetime.now()\n",
    "                for i in range(V):\n",
    "                    # matrix = reg*np.eye(D) + np.sum((fX[i,j]*np.outer(U[j], U[j]) for j in xrange(V)), axis=0)\n",
    "                    matrix = reg*np.eye(D) + (fX[i,:]*U.T).dot(U)\n",
    "                    # assert(np.abs(matrix - matrix2).sum() < 10e-5)\n",
    "                    vector = (fX[i,:]*(logX[i,:] - b[i] - c - mu)).dot(U)\n",
    "                    W[i] = np.linalg.solve(matrix, vector)\n",
    "                # print \"fast way took:\", (datetime.now() - t0)\n",
    "\n",
    "                # slow way\n",
    "                # t0 = datetime.now()\n",
    "                # for i in xrange(V):\n",
    "                #     matrix2 = reg*np.eye(D)\n",
    "                #     vector2 = 0\n",
    "                #     for j in xrange(V):\n",
    "                #         matrix2 += fX[i,j]*np.outer(U[j], U[j])\n",
    "                #         vector2 += fX[i,j]*(logX[i,j] - b[i] - c[j])*U[j]\n",
    "                # print \"slow way took:\", (datetime.now() - t0)\n",
    "\n",
    "                    # assert(np.abs(matrix - matrix2).sum() < 10e-5)\n",
    "                    # assert(np.abs(vector - vector2).sum() < 10e-5)\n",
    "                    # W[i] = np.linalg.solve(matrix, vector)\n",
    "                # print \"updated W\"\n",
    "\n",
    "                # update b\n",
    "                for i in range(V):\n",
    "                    denominator = fX[i,:].sum()\n",
    "                    # assert(denominator > 0)\n",
    "                    numerator = fX[i,:].dot(logX[i,:] - W[i].dot(U.T) - c - mu)\n",
    "                    # for j in xrange(V):\n",
    "                    #     numerator += fX[i,j]*(logX[i,j] - W[i].dot(U[j]) - c[j])\n",
    "                    b[i] = numerator / denominator / (1 + reg)\n",
    "                # print \"updated b\"\n",
    "\n",
    "                # update U\n",
    "                for j in range(V):\n",
    "                    # matrix = reg*np.eye(D) + np.sum((fX[i,j]*np.outer(W[i], W[i]) for i in xrange(V)), axis=0)\n",
    "                    matrix = reg*np.eye(D) + (fX[:,j]*W.T).dot(W)\n",
    "                    # assert(np.abs(matrix - matrix2).sum() < 10e-8)\n",
    "                    vector = (fX[:,j]*(logX[:,j] - b - c[j] - mu)).dot(W)\n",
    "                    # matrix = reg*np.eye(D)\n",
    "                    # vector = 0\n",
    "                    # for i in xrange(V):\n",
    "                    #     matrix += fX[i,j]*np.outer(W[i], W[i])\n",
    "                    #     vector += fX[i,j]*(logX[i,j] - b[i] - c[j])*W[i]\n",
    "                    U[j] = np.linalg.solve(matrix, vector)\n",
    "                # print \"updated U\"\n",
    "\n",
    "                # update c\n",
    "                for j in range(V):\n",
    "                    denominator = fX[:,j].sum()\n",
    "                    numerator = fX[:,j].dot(logX[:,j] - W.dot(U[j]) - b  - mu)\n",
    "                    # for i in xrange(V):\n",
    "                    #     numerator += fX[i,j]*(logX[i,j] - W[i].dot(U[j]) - b[i])\n",
    "                    c[j] = numerator / denominator / (1 + reg)\n",
    "                # print \"updated c\"\n",
    "\n",
    "        self.W = W\n",
    "        self.U = U\n",
    "\n",
    "        plt.plot(costs)\n",
    "        plt.show()\n",
    "\n",
    "    def save(self, fn):\n",
    "        # function word_analogies expects a (V,D) matrx and a (D,V) matrix\n",
    "        arrays = [self.W, self.U.T]\n",
    "        np.savez(fn, *arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max in X: 194664.84762\n",
      "max in f(X): 1.0\n",
      "max in log(X): 12.1790397657\n",
      "time to build co-occurrence matrix: 0:00:02.860000\n",
      "epoch: 0 cost: 926278.29206\n",
      "epoch: 1 cost: 406004.531001\n",
      "epoch: 2 cost: 575635.812685\n",
      "epoch: 3 cost: 430903.217361\n",
      "epoch: 4 cost: 439405.274157\n",
      "epoch: 5 cost: 441729.168145\n",
      "epoch: 6 cost: 391250.004177\n",
      "epoch: 7 cost: 345239.725872\n",
      "epoch: 8 cost: 260739.106723\n",
      "epoch: 9 cost: 376216.800319\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSNwUGiIiEjeFhoiIxE2hISIicfv/bC5RuDnxwm0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x136f49e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** concat: True\n",
      "We.shape: (5001, 200)\n",
      "closest match by Euclidean distance: apparent\n",
      "king - man = apparent - woman\n",
      "closest match by cosine distance: apparent\n",
      "king - man = apparent - woman\n",
      "We.shape: (5001, 200)\n",
      "closest match by Euclidean distance: introduced\n",
      "france - paris = introduced - london\n",
      "closest match by cosine distance: sba\n",
      "france - paris = sba - london\n",
      "We.shape: (5001, 200)\n",
      "closest match by Euclidean distance: fascinating\n",
      "france - paris = fascinating - rome\n",
      "closest match by cosine distance: fascinating\n",
      "france - paris = fascinating - rome\n",
      "We.shape: (5001, 200)\n",
      "closest match by Euclidean distance: consequence\n",
      "paris - france = consequence - italy\n",
      "closest match by cosine distance: closely\n",
      "paris - france = closely - italy\n",
      "We.shape: (5001, 200)\n",
      "closest match by Euclidean distance: extent\n",
      "france - french = extent - english\n",
      "closest match by cosine distance: root\n",
      "france - french = root - english\n",
      "We.shape: (5001, 200)\n",
      "closest match by Euclidean distance: tournament\n",
      "japan - japanese = tournament - chinese\n",
      "closest match by cosine distance: tournament\n",
      "japan - japanese = tournament - chinese\n",
      "We.shape: (5001, 200)\n",
      "closest match by Euclidean distance: emphasize\n",
      "japan - japanese = emphasize - italian\n",
      "closest match by cosine distance: entry\n",
      "japan - japanese = entry - italian\n",
      "We.shape: (5001, 200)\n",
      "closest match by Euclidean distance: tournament\n",
      "japan - japanese = tournament - australian\n",
      "closest match by cosine distance: tape\n",
      "japan - japanese = tape - australian\n",
      "We.shape: (5001, 200)\n",
      "closest match by Euclidean distance: crack\n",
      "december - november = crack - june\n",
      "closest match by cosine distance: crack\n",
      "december - november = crack - june\n",
      "** concat: False\n",
      "closest match by Euclidean distance: apparent\n",
      "king - man = apparent - woman\n",
      "closest match by cosine distance: apparent\n",
      "king - man = apparent - woman\n",
      "closest match by Euclidean distance: tragic\n",
      "france - paris = tragic - london\n",
      "closest match by cosine distance: tragic\n",
      "france - paris = tragic - london\n",
      "closest match by Euclidean distance: vermont\n",
      "france - paris = vermont - rome\n",
      "closest match by cosine distance: vermont\n",
      "france - paris = vermont - rome\n",
      "closest match by Euclidean distance: skill\n",
      "paris - france = skill - italy\n",
      "closest match by cosine distance: skill\n",
      "paris - france = skill - italy\n",
      "closest match by Euclidean distance: cope\n",
      "france - french = cope - english\n",
      "closest match by cosine distance: mirror\n",
      "france - french = mirror - english\n",
      "closest match by Euclidean distance: sympathetic\n",
      "japan - japanese = sympathetic - chinese\n",
      "closest match by cosine distance: channels\n",
      "japan - japanese = channels - chinese\n",
      "closest match by Euclidean distance: david\n",
      "japan - japanese = david - italian\n",
      "closest match by cosine distance: david\n",
      "japan - japanese = david - italian\n",
      "closest match by Euclidean distance: tape\n",
      "japan - japanese = tape - australian\n",
      "closest match by cosine distance: tape\n",
      "japan - japanese = tape - australian\n",
      "closest match by Euclidean distance: appearance\n",
      "december - november = appearance - june\n",
      "closest match by cosine distance: appearance\n",
      "december - november = appearance - june\n"
     ]
    }
   ],
   "source": [
    "def main(we_file, w2i_file, use_brown=True, n_files=50):\n",
    "    if use_brown:\n",
    "        cc_matrix = \"cc_matrix_brown.npy\"\n",
    "    else:\n",
    "        cc_matrix = \"cc_matrix_%s.npy\" % n_files\n",
    "\n",
    "    # hacky way of checking if we need to re-load the raw data or not\n",
    "    # remember, only the co-occurrence matrix is needed for training\n",
    "    if os.path.exists(cc_matrix):\n",
    "        with open(w2i_file) as f:\n",
    "            word2idx = json.load(f)\n",
    "        sentences = [] # dummy - we won't actually use it\n",
    "    else:\n",
    "        if use_brown:\n",
    "            keep_words = set([\n",
    "                'king', 'man', 'woman',\n",
    "                'france', 'paris', 'london', 'rome', 'italy', 'britain', 'england',\n",
    "                'french', 'english', 'japan', 'japanese', 'chinese', 'italian',\n",
    "                'australia', 'australian', 'december', 'november', 'june',\n",
    "                'january', 'february', 'march', 'april', 'may', 'july', 'august',\n",
    "                'september', 'october',\n",
    "            ])\n",
    "            sentences, word2idx = get_sentences_with_word2idx_limit_vocab(n_vocab=5000, keep_words=keep_words)\n",
    "        else:\n",
    "            sentences, word2idx = get_wikipedia_data(n_files=n_files, n_vocab=2000)\n",
    "        \n",
    "        with open(w2i_file, 'w') as f:\n",
    "            json.dump(word2idx, f)\n",
    "\n",
    "    V = len(word2idx)\n",
    "    model = Glove(100, V, 10)\n",
    "    # model.fit(sentences, cc_matrix=cc_matrix, epochs=20) # ALS\n",
    "    model.fit(\n",
    "        sentences,\n",
    "        cc_matrix=cc_matrix,\n",
    "        learning_rate=3*10e-5,\n",
    "        reg=0.1,\n",
    "        epochs=10,\n",
    "        gd=True,\n",
    "        use_theano=False,\n",
    "        use_tensorflow=True,\n",
    "    )\n",
    "    model.save(we_file)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # we = 'glove_model_50.npz'\n",
    "    # w2i = 'glove_word2idx_50.json'\n",
    "    we = 'glove_model_brown.npz'\n",
    "    w2i = 'glove_word2idx_brown.json'\n",
    "    main(we, w2i, use_brown=True)\n",
    "    for concat in (True, False):\n",
    "        print (\"** concat:\", concat)\n",
    "        find_analogies('king', 'man', 'woman', concat, we, w2i)\n",
    "        find_analogies('france', 'paris', 'london', concat, we, w2i)\n",
    "        find_analogies('france', 'paris', 'rome', concat, we, w2i)\n",
    "        find_analogies('paris', 'france', 'italy', concat, we, w2i)\n",
    "        find_analogies('france', 'french', 'english', concat, we, w2i)\n",
    "        find_analogies('japan', 'japanese', 'chinese', concat, we, w2i)\n",
    "        find_analogies('japan', 'japanese', 'italian', concat, we, w2i)\n",
    "        find_analogies('japan', 'japanese', 'australian', concat, we, w2i)\n",
    "        find_analogies('december', 'november', 'june', concat, we, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

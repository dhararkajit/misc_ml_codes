{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import os\n",
    "import operator\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weight(Mi, Mo):\n",
    "    return np.random.randn(Mi, Mo) / np.sqrt(Mi + Mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_parity_pairs(nbit):\n",
    "    # total number of samples (Ntotal) will be a multiple of 100\n",
    "    # why did I make it this way? I don't remember.\n",
    "    N = 2**nbit\n",
    "    remainder = 100 - (N % 100)\n",
    "    Ntotal = N + remainder\n",
    "    X = np.zeros((Ntotal, nbit))\n",
    "    Y = np.zeros(Ntotal)\n",
    "    for ii in range(Ntotal):\n",
    "        i = ii % N\n",
    "        # now generate the ith sample\n",
    "        for j in range(nbit):\n",
    "            if i % (2**(j+1)) != 0:\n",
    "                i -= 2**j\n",
    "                X[ii,j] = 1\n",
    "        Y[ii] = X[ii].sum() % 2\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_parity_pairs_with_sequence_labels(nbit):\n",
    "    X, Y = all_parity_pairs(nbit)\n",
    "    N, t = X.shape\n",
    "\n",
    "    # we want every time step to have a label\n",
    "    Y_t = np.zeros(X.shape, dtype=np.int32)\n",
    "    for n in range(N):\n",
    "        ones_count = 0\n",
    "        for i in range(t):\n",
    "            if X[n,i] == 1:\n",
    "                ones_count += 1\n",
    "            if ones_count % 2 == 1:\n",
    "                Y_t[n,i] = 1\n",
    "\n",
    "    X = X.reshape(N, t, 1).astype(np.float32)\n",
    "    return X, Y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    return s.translate(None, string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_robert_frost():\n",
    "    word2idx = {'START': 0, 'END': 1}\n",
    "    current_idx = 2\n",
    "    sentences = []\n",
    "    for line in open('../hmm_class/robert_frost.txt'):\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            tokens = remove_punctuation(line.lower()).split()\n",
    "            sentence = []\n",
    "            for t in tokens:\n",
    "                if t not in word2idx:\n",
    "                    word2idx[t] = current_idx\n",
    "                    current_idx += 1\n",
    "                idx = word2idx[t]\n",
    "                sentence.append(idx)\n",
    "            sentences.append(sentence)\n",
    "    return sentences, word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_tokenizer(s):\n",
    "    s = remove_punctuation(s)\n",
    "    s = s.lower() # downcase\n",
    "    return s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wikipedia_data(n_files, n_vocab, by_paragraph=False):\n",
    "    prefix = '../large_files/'\n",
    "    if not os.path.exists(prefix):\n",
    "        print (\"Are you sure you've downloaded, converted, and placed the Wikipedia data into the proper folder?\")\n",
    "        print (\"I'm looking for a folder called large_files, adjacent to the class folder, but it does not exist.\")\n",
    "        print (\"Please download the data from https://dumps.wikimedia.org/\")\n",
    "        print (\"Quitting...\")\n",
    "        exit()\n",
    "\n",
    "    input_files = [f for f in os.listdir(prefix) if f.startswith('enwiki') and f.endswith('txt')]\n",
    "\n",
    "    if len(input_files) == 0:\n",
    "        print (\"Looks like you don't have any data files, or they're in the wrong location.\")\n",
    "        print (\"Please download the data from https://dumps.wikimedia.org/\")\n",
    "        print (\"Quitting...\")\n",
    "        exit()\n",
    "\n",
    "    # return variables\n",
    "    sentences = []\n",
    "    word2idx = {'START': 0, 'END': 1}\n",
    "    idx2word = ['START', 'END']\n",
    "    current_idx = 2\n",
    "    word_idx_count = {0: float('inf'), 1: float('inf')}\n",
    "\n",
    "    if n_files is not None:\n",
    "        input_files = input_files[:n_files]\n",
    "\n",
    "    for f in input_files:\n",
    "        print(\"reading:\", f)\n",
    "        for line in open(prefix + f):\n",
    "            line = line.strip()\n",
    "            # don't count headers, structured data, lists, etc...\n",
    "            if line and line[0] not in ('[', '*', '-', '|', '=', '{', '}'):\n",
    "                if by_paragraph:\n",
    "                    sentence_lines = [line]\n",
    "                else:\n",
    "                    sentence_lines = line.split('. ')\n",
    "                for sentence in sentence_lines:\n",
    "                    tokens = my_tokenizer(sentence)\n",
    "                    for t in tokens:\n",
    "                        if t not in word2idx:\n",
    "                            word2idx[t] = current_idx\n",
    "                            idx2word.append(t)\n",
    "                            current_idx += 1\n",
    "                        idx = word2idx[t]\n",
    "                        word_idx_count[idx] = word_idx_count.get(idx, 0) + 1\n",
    "                    sentence_by_idx = [word2idx[t] for t in tokens]\n",
    "                    sentences.append(sentence_by_idx)\n",
    "\n",
    "    # restrict vocab size\n",
    "    sorted_word_idx_count = sorted(word_idx_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    word2idx_small = {}\n",
    "    new_idx = 0\n",
    "    idx_new_idx_map = {}\n",
    "    for idx, count in sorted_word_idx_count[:n_vocab]:\n",
    "        word = idx2word[idx]\n",
    "        print(word, count)\n",
    "        word2idx_small[word] = new_idx\n",
    "        idx_new_idx_map[idx] = new_idx\n",
    "        new_idx += 1\n",
    "    # let 'unknown' be the last token\n",
    "    word2idx_small['UNKNOWN'] = new_idx \n",
    "    unknown = new_idx\n",
    "\n",
    "    assert('START' in word2idx_small)\n",
    "    assert('END' in word2idx_small)\n",
    "    assert('king' in word2idx_small)\n",
    "    assert('queen' in word2idx_small)\n",
    "    assert('man' in word2idx_small)\n",
    "    assert('woman' in word2idx_small)\n",
    "\n",
    "    # map old idx to new idx\n",
    "    sentences_small = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) > 1:\n",
    "            new_sentence = [idx_new_idx_map[idx] if idx in idx_new_idx_map else unknown for idx in sentence]\n",
    "            sentences_small.append(new_sentence)\n",
    "\n",
    "    return sentences_small, word2idx_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tags(s):\n",
    "    tuples = pos_tag(word_tokenize(s))\n",
    "    return [y for x, y in tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_poetry_classifier_data(samples_per_class, load_cached=True, save_cached=True):\n",
    "    datafile = 'poetry_classifier_data.npz'\n",
    "    if load_cached and os.path.exists(datafile):\n",
    "        npz = np.load(datafile)\n",
    "        X = npz['arr_0']\n",
    "        Y = npz['arr_1']\n",
    "        V = int(npz['arr_2'])\n",
    "        return X, Y, V\n",
    "\n",
    "    word2idx = {}\n",
    "    current_idx = 0\n",
    "    X = []\n",
    "    Y = []\n",
    "    for fn, label in zip(('../hmm_class/edgar_allan_poe.txt', '../hmm_class/robert_frost.txt'), (0, 1)):\n",
    "        count = 0\n",
    "        for line in open(fn):\n",
    "            line = line.rstrip()\n",
    "            if line:\n",
    "                print(line)\n",
    "                # tokens = remove_punctuation(line.lower()).split()\n",
    "                tokens = get_tags(line)\n",
    "                if len(tokens) > 1:\n",
    "                    # scan doesn't work nice here, technically could fix...\n",
    "                    for token in tokens:\n",
    "                        if token not in word2idx:\n",
    "                            word2idx[token] = current_idx\n",
    "                            current_idx += 1\n",
    "                    sequence = np.array([word2idx[w] for w in tokens])\n",
    "                    X.append(sequence)\n",
    "                    Y.append(label)\n",
    "                    count += 1\n",
    "                    print(count)\n",
    "                    # quit early because the tokenizer is very slow\n",
    "                    if count >= samples_per_class:\n",
    "                        break\n",
    "    if save_cached:\n",
    "        np.savez(datafile, X, Y, current_idx)\n",
    "    return X, Y, current_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stock_data():\n",
    "    input_files = os.listdir('stock_data')\n",
    "    min_length = 2000\n",
    "\n",
    "    # first find the latest start date\n",
    "    # so that each time series can start at the same time\n",
    "    max_min_date = datetime(2000, 1, 1)\n",
    "    line_counts = {}\n",
    "    for f in input_files:\n",
    "        n = 0\n",
    "        for line in open('stock_data/%s' % f):\n",
    "            # pass\n",
    "            n += 1\n",
    "        line_counts[f] = n\n",
    "        if n > min_length:\n",
    "            # else we'll ignore this symbol, too little data\n",
    "            # print 'stock_data/%s' % f, 'num lines:', n\n",
    "            last_line = line\n",
    "            date = line.split(',')[0]\n",
    "            date = datetime.strptime(date, '%Y-%m-%d')\n",
    "            if date > max_min_date:\n",
    "                max_min_date = date\n",
    "\n",
    "    print(\"max min date:\", max_min_date)\n",
    "\n",
    "    # now collect the data up to min date\n",
    "    all_binary_targets = []\n",
    "    all_prices = []\n",
    "    for f in input_files:\n",
    "        if line_counts[f] > min_length:\n",
    "            prices = []\n",
    "            binary_targets = []\n",
    "            first = True\n",
    "            last_price = 0\n",
    "            for line in open('stock_data/%s' % f):\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "                date, price = line.split(',')[:2]\n",
    "                date = datetime.strptime(date, '%Y-%m-%d')\n",
    "                if date < max_min_date:\n",
    "                    break\n",
    "                prices.append(float(price))\n",
    "                target = 1 if last_price < price else 0\n",
    "                binary_targets.append(target)\n",
    "                last_price = price\n",
    "            all_prices.append(prices)\n",
    "            all_binary_targets.append(binary_targets)\n",
    "\n",
    "    # D = number of symbols\n",
    "    # T = length of series\n",
    "    return np.array(all_prices).T, np.array(all_binary_targets).T # make it T x D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_analogies(w1, w2, w3, We, word2idx):\n",
    "    king = We[word2idx[w1]]\n",
    "    man = We[word2idx[w2]]\n",
    "    woman = We[word2idx[w3]]\n",
    "    v0 = king - man + woman\n",
    "\n",
    "    def dist1(a, b):\n",
    "        return np.linalg.norm(a - b)\n",
    "    def dist2(a, b):\n",
    "        return 1 - a.dot(b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "    for dist, name in [(dist1, 'Euclidean'), (dist2, 'cosine')]:\n",
    "        min_dist = float('inf')\n",
    "        best_word = ''\n",
    "        for word, idx in word2idx.items():\n",
    "            if word not in (w1, w2, w3):\n",
    "                v1 = We[idx]\n",
    "                d = dist(v0, v1)\n",
    "                if d < min_dist:\n",
    "                    min_dist = d\n",
    "                    best_word = word\n",
    "        print (\"closest match by\", name, \"distance:\", best_word)\n",
    "        print (1, \"-\", w2, \"=\", best_word, \"-\", w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
